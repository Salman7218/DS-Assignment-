{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cdf751-d2f6-4e8c-8d3b-4075aa5ed3d1",
   "metadata": {},
   "source": [
    "# Support Vector Machines-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634367b-5c72-44a5-8d04-2303d94e447a",
   "metadata": {},
   "source": [
    "**Q1. What is the mathematical formula for a linear SVM?**\n",
    "\n",
    "Ans.:The mathematical formula for a linear Support Vector Machine (SVM) is typically used for binary classification and can be expressed as follows:\n",
    "\n",
    "Given a training dataset with 'm' data points and 'n' features, where the input data is represented as x^(i) for i = 1, 2, ..., m, and the corresponding binary class labels as y^(i) where y^(i) is either -1 or 1, the linear SVM tries to find the optimal hyperplane defined by the weight vector (W) and bias term (b) that maximizes the margin between the two classes while minimizing the classification error. The mathematical formulation of a linear SVM is:\n",
    "\n",
    "**Minimize:** 1/2 * ||W||^2\n",
    "\n",
    "**Subject to:** y^(i) * (W * x^(i) + b) ≥ 1 for all i, where i = 1, 2, ..., m\n",
    "\n",
    "In this formulation:\n",
    "\n",
    "- W is the weight vector that defines the orientation of the hyperplane.\n",
    "- b is the bias term, also known as the intercept.\n",
    "- x^(i) represents the feature vector of the i-th data point.\n",
    "- y^(i) is the binary class label for the i-th data point.\n",
    "\n",
    "The objective is to find the values of W and b that minimize the norm of W (the length of the weight vector) while ensuring that all data points are correctly classified and lie on the correct side of the hyperplane, with a margin of at least 1. The value 1 in the inequality is a constant representing the margin; for linearly separable data, this constraint ensures that the margin is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51a33e-7afd-478a-8493-266c223bc767",
   "metadata": {},
   "source": [
    "**Q2. What is the objective function of a linear SVM?*\n",
    "\n",
    "Ans.: The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that maximizes the margin between two classes in a binary classification problem while minimizing the classification error. The objective function can be mathematically expressed as follows:\n",
    "\n",
    "**Minimize:** 1/2 * ||W||^2\n",
    "\n",
    "In this objective function:\n",
    "\n",
    "- W is the weight vector that defines the orientation of the hyperplane.\n",
    "- ||W|| represents the norm (magnitude or length) of the weight vector.\n",
    "\n",
    "The goal of this objective function is to minimize the square of the Euclidean norm (L2 norm) of the weight vector W. This effectively tries to maximize the margin between the classes. A larger margin indicates better separation between the classes and is associated with improved generalization and better classification performance.\n",
    "\n",
    "The SVM aims to find the values of W and b (the bias term) that minimize this objective function while satisfying the following constraint for each data point:\n",
    "\n",
    "**Subject to:** y^(i) * (W * x^(i) + b) ≥ 1 for all i, where i = 1, 2, ..., m\n",
    "\n",
    "In this constraint:\n",
    "\n",
    "- y^(i) is the binary class label for the i-th data point.\n",
    "- x^(i) represents the feature vector of the i-th data point.\n",
    "\n",
    "This constraint ensures that all data points are correctly classified and lie on the correct side of the hyperplane, with a margin of at least 1. The value 1 in the inequality represents the margin requirement.\n",
    "\n",
    "So, the SVM's objective function balances the trade-off between maximizing the margin (by minimizing the norm of W) and ensuring that all data points are correctly classified, thus achieving a well-generalized classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d879a-60c5-4809-9388-e2c0ea60e77f",
   "metadata": {},
   "source": [
    "**Q3. What is the kernel trick in SVM?**\n",
    "\n",
    "Ans.: The kernel trick is a fundamental concept in Support Vector Machines (SVMs) and is used to extend SVMs for nonlinear classification and regression problems. It allows SVMs to operate in a higher-dimensional feature space without explicitly computing the transformation of the input data into that space. This is accomplished by using a kernel function, which is a mathematical function that computes the dot product of data points in the higher-dimensional space without actually transforming the data into that space. The kernel function is applied to the original data in the input space, and the SVM operates as if it were working in the higher-dimensional space.\n",
    "\n",
    "In other words, the kernel trick allows SVMs to find complex, nonlinear decision boundaries in the input space by projecting the data into a higher-dimensional space, where the data may become linearly separable, and then applying the SVM's linear classification or regression algorithm.\n",
    "\n",
    "Some common kernel functions used in SVMs include:\n",
    "\n",
    "1. **Linear Kernel**: This is the same as the standard linear SVM and is used for linearly separable data.\n",
    "\n",
    "2. **Polynomial Kernel**: This kernel can capture polynomial relationships in the data. It is defined by a parameter d (degree), which determines the degree of the polynomial.\n",
    "\n",
    "3. **Radial Basis Function (RBF) Kernel**: Also known as the Gaussian kernel, it is commonly used for capturing complex, non-linear relationships. It is defined by a parameter γ (gamma) that controls the shape of the kernel.\n",
    "\n",
    "4. **Sigmoid Kernel**: This kernel is used to model sigmoidal (S-shaped) decision boundaries.\n",
    "\n",
    "5. **Custom Kernels**: You can also define custom kernels based on the specific problem you are trying to solve.\n",
    "\n",
    "The choice of kernel function and its hyperparameters can significantly impact the SVM's performance. The correct selection of the kernel and tuning of hyperparameters can make SVMs highly effective for various tasks. However, it's important to note that selecting the right kernel and hyperparameters can be a non-trivial task, and it may require experimentation and cross-validation to determine the best settings for a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c8e3b-e3fe-4cbc-bdd1-1e2104a32c4f",
   "metadata": {},
   "source": [
    "**Q4. What is the role of support vectors in SVM Explain with example**\n",
    "\n",
    "Ans.: In a Support Vector Machine (SVM), support vectors are the data points that are closest to the decision boundary (hyperplane) and have the most influence on defining the position and orientation of the hyperplane. These are the data points that are critical in determining the margin and, in turn, the performance of the SVM.\n",
    "\n",
    "The role of support vectors in SVM can be explained with an example:\n",
    "\n",
    "Let's consider a simple two-class classification problem in two-dimensional space (2D). You have two classes, A (shown in red) and B (shown in blue), and you want to find a linear decision boundary to separate them:\n",
    "\n",
    "```\n",
    "    A     B\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "------------------  (Decision Boundary)\n",
    "```\n",
    "\n",
    "In this case, the decision boundary is represented by a line, and the support vectors are the data points that are closest to this line, with the smallest margin between them and the decision boundary. These support vectors are critical in determining the position and orientation of the decision boundary. If you move or modify the decision boundary, it will affect these support vectors first.\n",
    "\n",
    "For example, the support vectors for this problem might be the data points that are located close to the decision boundary and are circled in the diagram below:\n",
    "\n",
    "```\n",
    "    A     B\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "    |     |\n",
    "------------------  (Decision Boundary)\n",
    "               O  O\n",
    "              O    O\n",
    "             O      O\n",
    "            O        O\n",
    "           O          O\n",
    "          O            O\n",
    "         O              O\n",
    "```\n",
    "\n",
    "These support vectors are crucial because they \"support\" the decision boundary, and the margin of the SVM is defined by the distance between the decision boundary and these support vectors. A larger margin indicates better generalization and better performance of the SVM.\n",
    "\n",
    "In summary, support vectors play a central role in SVMs by helping determine the position and orientation of the decision boundary and the margin, which is essential for achieving good classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84354818-96bd-46c7-9040-81c83e0b25f6",
   "metadata": {},
   "source": [
    "**Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?**\n",
    "\n",
    "Ans.: Sure, I'll illustrate the concepts of the hyperplane, marginal plane, soft margin, and hard margin in SVM with examples and graphs.\n",
    "\n",
    "**1. Hyperplane:**\n",
    "\n",
    "The hyperplane is the decision boundary that separates data points of different classes in an SVM. In a two-dimensional space, it's a line, and in a three-dimensional space, it's a plane. Let's consider a simple 2D example:\n",
    "\n",
    "Suppose we have two classes, A (red) and B (blue), and the data is linearly separable. The hyperplane is the line that separates the two classes:\n",
    "\n",
    "```\n",
    "  A   | \n",
    "      |\n",
    "      |      HYP\n",
    "      |          LANE\n",
    "  B   |\n",
    "----------------------\n",
    "```\n",
    "\n",
    "In this example, the hyperplane is a line that effectively separates class A from class B.\n",
    "\n",
    "**2. Marginal Plane:**\n",
    "\n",
    "The marginal plane represents the support vectors that determine the margin in an SVM. The margin is the distance between the hyperplane and the closest support vectors. Let's add support vectors to our previous example:\n",
    "\n",
    "```\n",
    "  A   |   O   O\n",
    "      |\n",
    "      |      HYP\n",
    "      |          LANE\n",
    "  B   |   O   O\n",
    "----------------------\n",
    "```\n",
    "\n",
    "The marginal plane consists of the support vectors (the points marked with \"O\" above) that are closest to the hyperplane. These support vectors define the margin.\n",
    "\n",
    "**3. Hard Margin:**\n",
    "\n",
    "In a hard-margin SVM, the goal is to find a hyperplane that perfectly separates the two classes with no misclassifications. Here's a representation:\n",
    "\n",
    "```\n",
    "  A   |   O   O\n",
    "      |\n",
    "      |      HYP\n",
    "      |          LANE\n",
    "  B   |   O   O\n",
    "----------------------\n",
    "```\n",
    "\n",
    "In this case, the hyperplane separates the classes A and B without any overlap or misclassification. A hard margin SVM only works when the data is perfectly linearly separable. However, real-world data is often not perfectly separable, which is where soft margin SVM comes in.\n",
    "\n",
    "**4. Soft Margin:**\n",
    "\n",
    "In a soft-margin SVM, the goal is to find a hyperplane that allows for some margin violations or misclassifications to account for noisy or overlapping data. Here's a representation:\n",
    "\n",
    "```\n",
    "  A   |   O   O\n",
    "      |  \n",
    "      |      HYP\n",
    "      |          LANE\n",
    "  B   |   O   O\n",
    "----------------------\n",
    "```\n",
    "\n",
    "In this example, the hyperplane still tries to separate the classes as best as possible while allowing for a small number of points to be inside the margin or on the wrong side of the decision boundary. The balance between maximizing the margin and allowing for margin violations is controlled by a parameter called the \"softness\" or \"C\" parameter in the SVM algorithm.\n",
    "\n",
    "Soft margin SVMs are more robust to real-world data, which is often not perfectly separable, as they can tolerate some degree of misclassification while still trying to find the best possible decision boundary.\n",
    "\n",
    "These examples and representations should help you understand the concepts of the hyperplane, marginal plane, soft margin, and hard margin in Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9b862-5473-463e-b3d9-9d80cc54ead4",
   "metadata": {},
   "source": [
    "**Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.**\n",
    "\n",
    "Ans.: Certainly! Here's a step-by-step implementation of a linear SVM classifier using the Iris dataset from scikit-learn. We'll load the dataset, split it into a training set and a testing set, train a linear SVM classifier, compute its accuracy, plot decision boundaries, and experiment with different values of the regularization parameter 'C.'\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "# You can experiment with different values of the regularization parameter C\n",
    "C = 1.0  # Change this value to experiment with different C values\n",
    "clf = SVC(kernel='linear', C=C)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot decision boundaries of the trained model\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot decision boundary and margin\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, levels=[-1, 0, 1], alpha=0.5, cmap=plt.cm.Paired)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.title(f\"Linear SVM Decision Boundaries (C = {C})\")\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Make sure you have scikit-learn and matplotlib installed in your Python environment. You can change the value of `C` to experiment with different regularization strengths. Higher values of `C` make the margin narrower but may lead to overfitting, while lower values make the margin wider but may lead to underfitting.\n",
    "\n",
    "For the bonus task of implementing a linear SVM classifier from scratch, you would need to write a substantial amount of code, which is beyond the scope of a simple response. However, you can find Python libraries like LIBSVM and SVMLight that provide the implementations of SVMs. You can also explore the scikit-learn source code to see the implementation of SVMs in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e11362-fbb0-4ae1-b2bf-14919e73822c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
