{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f1d12c5-3082-4623-addd-570e6471c209",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1ad3f-acae-401b-bcdd-99d17ff45af8",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n",
    "\n",
    "Ans.: **Simple Linear Regression**:\n",
    "\n",
    "Simple linear regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (response). It assumes that this relationship can be represented by a straight line (hence \"linear\"). The goal is to find the best-fitting line that minimizes the sum of the squared differences between the observed data points and the predicted values on the line.\n",
    "\n",
    "**Example of Simple Linear Regression**:\n",
    "\n",
    "Imagine you want to predict a person's salary (dependent variable) based on their years of experience (independent variable). Each data point in your dataset consists of a person's years of experience and their corresponding salary. You can use simple linear regression to find a line that best represents this relationship and can make predictions about a person's salary given their years of experience.\n",
    "\n",
    "**Multiple Linear Regression**:\n",
    "\n",
    "Multiple linear regression is an extension of simple linear regression, but it deals with multiple independent variables. In this case, you have two or more independent variables that are used to predict a single dependent variable. The model assumes a linear relationship between the independent variables and the dependent variable.\n",
    "\n",
    "**Example of Multiple Linear Regression**:\n",
    "\n",
    "Suppose you want to predict a house's sale price (dependent variable) based on various factors such as the number of bedrooms, square footage, and neighborhood crime rate (independent variables). In this case, you have multiple independent variables, and you use multiple linear regression to build a model that accounts for the combined effect of these variables on the house's sale price. The regression equation would look something like:\n",
    "\n",
    "Sale Price = β0 + β1 * Bedrooms + β2 * Square Footage + β3 * Crime Rate + ε\n",
    "\n",
    "Here, β0, β1, β2, β3 are the coefficients to be estimated, and ε represents the error term.\n",
    "\n",
    "In summary, the key difference between simple and multiple linear regression is the number of independent variables. Simple linear regression deals with one independent variable, while multiple linear regression deals with two or more independent variables to predict a single dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7323b4c-5d83-45b1-8d13-949076d0180a",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n",
    "\n",
    "Ans.: Linear regression is a powerful statistical tool, but it relies on several assumptions to provide valid and reliable results. Violations of these assumptions can lead to inaccurate or biased conclusions. Here are the key assumptions of linear regression and methods to check whether they hold in a given dataset:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable should be linear. You can check this assumption by creating scatterplots of the independent variables against the dependent variable. If the points on the scatterplots form a reasonably straight line, the linearity assumption may hold. You can also use residual plots to assess linearity.\n",
    "\n",
    "2. **Independence of Errors**: The errors (residuals) should be independent of each other. This means that the value of the error for one data point should not depend on the value of the error for another data point. You can check this assumption using autocorrelation plots of the residuals. If there is a pattern in the autocorrelation, it suggests a violation of this assumption.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the errors should be constant across all levels of the independent variables. To check for homoscedasticity, you can create a plot of residuals against the predicted values. If the spread of residuals remains fairly consistent as predicted values change, the assumption is likely met. If the spread of residuals widens or narrows systematically, it suggests heteroscedasticity.\n",
    "\n",
    "4. **Normality of Errors**: The errors should be normally distributed. You can examine this assumption by creating a histogram or a Q-Q plot of the residuals. If the distribution is roughly bell-shaped and follows a normal curve, the assumption is likely met. In cases of non-normality, you may consider transforming the data or using robust regression techniques.\n",
    "\n",
    "5. **No or Little Multicollinearity**: If you have multiple independent variables in a multiple linear regression, they should not be highly correlated with each other. Multicollinearity can lead to unstable and unreliable coefficient estimates. You can calculate correlation coefficients between independent variables and assess the variance inflation factor (VIF) to detect multicollinearity.\n",
    "\n",
    "6. **Zero Conditional Mean**: The expected value of the errors should be zero at every level of the independent variables. This means that, on average, the model is correctly specified. You can check this by plotting the residuals against the independent variables. If the residuals show a systematic pattern, it may indicate a violation of this assumption.\n",
    "\n",
    "To address violations of these assumptions, you can consider the following actions:\n",
    "\n",
    "- Transform the data: If the assumptions are violated, transforming the data or the dependent variable may help meet the assumptions.\n",
    "\n",
    "- Use robust regression techniques: Robust regression methods are less sensitive to violations of some assumptions, such as normality and heteroscedasticity.\n",
    "\n",
    "- Include relevant variables: Adding or removing independent variables from the model can help address issues like multicollinearity and conditional mean violations.\n",
    "\n",
    "- Use non-linear models: In cases where the relationship is not truly linear, you may need to consider nonlinear regression models.\n",
    "\n",
    "It's essential to check these assumptions to ensure the validity of your linear regression analysis and make informed decisions about your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75272fbe-e16b-484d-949a-d78e2346f75c",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n",
    "\n",
    "Ans.: In a linear regression model, you typically have an equation in the form:\n",
    "\n",
    "\\[Y = \\beta_0 + \\beta_1X + \\varepsilon\\]\n",
    "\n",
    "Where:\n",
    "- \\(Y\\) is the dependent variable you're trying to predict.\n",
    "- \\(X\\) is the independent variable used for prediction.\n",
    "- \\(\\beta_0\\) is the intercept (also called the constant or the y-intercept).\n",
    "- \\(\\beta_1\\) is the slope (also called the coefficient or the regression coefficient).\n",
    "- \\(\\varepsilon\\) represents the error term.\n",
    "\n",
    "Here's how you interpret the slope and intercept in a linear regression model using a real-world scenario:\n",
    "\n",
    "**Scenario**: Let's say you want to predict a person's weight (\\(Y\\)) based on their height (\\(X\\)). You have a dataset of heights and weights for a sample of individuals.\n",
    "\n",
    "1. **Intercept (\\(\\beta_0\\))**: The intercept represents the value of the dependent variable when the independent variable is zero. In our example, it would be the predicted weight of a person when their height is zero. However, this interpretation might not make sense in many cases. In most real-world scenarios, an interpretation of \\(\\beta_0\\) isn't meaningful, as it often doesn't correspond to any real-world situation.\n",
    "\n",
    "2. **Slope (\\(\\beta_1\\))**: The slope represents the change in the dependent variable for a one-unit change in the independent variable. In our example, it's the change in weight for a one-unit change in height. So, if \\(\\beta_1\\) is, for instance, 3.5, it means that, on average, for every one-inch increase in height, a person's weight is expected to increase by 3.5 pounds. Conversely, if \\(\\beta_1\\) were -3.5, it would mean that, on average, for every one-inch increase in height, a person's weight is expected to decrease by 3.5 pounds.\n",
    "\n",
    "It's important to remember that these interpretations are based on the simplifying assumptions of the linear regression model and that real-world relationships can be more complex. Additionally, the interpretation of the intercept (\\(\\beta_0\\)) can be problematic in many cases because it often doesn't have a meaningful real-world interpretation. In practice, the focus is usually on the slope (\\(\\beta_1\\)), which quantifies the effect of the independent variable on the dependent variable.\n",
    "\n",
    "So, in the context of our height and weight example, the slope (\\(\\beta_1\\)) tells you how much weight you'd expect a person to gain or lose for each one-inch change in height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e3166-69d6-4af0-82ab-6ea2cbb0208e",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Ans.: **Gradient Descent** is an optimization algorithm used to find the minimum of a function, specifically in the context of machine learning, to minimize the error or cost function associated with a model. It's a fundamental technique in training machine learning models, particularly in deep learning and linear regression.\n",
    "\n",
    "The basic idea behind gradient descent is to iteratively update the parameters of a model to minimize a cost function by moving in the direction of the steepest descent (negative gradient) of the cost function. Here's a simplified step-by-step explanation of how gradient descent works in machine learning:\n",
    "\n",
    "1. **Initialization**: Start with an initial guess for the model's parameters, often set randomly.\n",
    "\n",
    "2. **Compute the Gradient**: Calculate the gradient (vector of partial derivatives) of the cost function with respect to each model parameter. This gradient points in the direction of the steepest increase in the cost function.\n",
    "\n",
    "3. **Update Parameters**: Adjust the model's parameters in the opposite direction of the gradient to reduce the cost. The update rule is typically of the form:\n",
    "   \n",
    "   \\[ \\text{New Parameter} = \\text{Old Parameter} - \\text{Learning Rate} \\times \\text{Gradient} \\]\n",
    "\n",
    "   The learning rate is a hyperparameter that determines the size of the steps you take during each iteration. It influences the convergence and stability of the optimization process. If it's too large, you might overshoot the minimum; if it's too small, convergence can be slow.\n",
    "\n",
    "4. **Repeat Steps 2 and 3**: Continue computing gradients and updating parameters for a fixed number of iterations or until the cost function converges to a minimum.\n",
    "\n",
    "5. **Convergence Check**: You can check for convergence by monitoring the change in the cost function over iterations. If the change is below a certain threshold or if you've reached a maximum number of iterations, you can stop the optimization.\n",
    "\n",
    "**Use in Machine Learning**:\n",
    "\n",
    "Gradient descent is a crucial component in various machine learning algorithms, including:\n",
    "\n",
    "1. **Linear Regression**: Gradient descent is used to find the optimal coefficients that minimize the mean squared error.\n",
    "\n",
    "2. **Logistic Regression**: It's used to optimize the parameters of logistic regression models for binary and multi-class classification.\n",
    "\n",
    "3. **Neural Networks**: In deep learning, gradient descent (often variants like stochastic gradient descent) is used to train neural networks by adjusting the weights and biases to minimize the error or loss function.\n",
    "\n",
    "4. **Support Vector Machines**: Gradient descent is used to optimize the parameters in support vector machines to find the best hyperplane.\n",
    "\n",
    "5. **Recommendation Systems**: It's employed in collaborative filtering and matrix factorization to optimize the model parameters.\n",
    "\n",
    "6. **Clustering and Dimensionality Reduction**: Gradient-based methods are used in optimizing objective functions for clustering (e.g., K-Means) and dimensionality reduction (e.g., t-SNE).\n",
    "\n",
    "In practice, various optimization techniques, such as mini-batch gradient descent, momentum, RMSprop, and Adam, are employed to enhance the convergence and stability of gradient descent in machine learning. These methods adapt the learning rate during training and often provide faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b23fb2-87f3-4df5-b83f-0dbbc447a8f2",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Ans.: **Multiple Linear Regression** is a statistical modeling technique used to analyze the relationship between a dependent variable (target) and two or more independent variables (predictors). It's an extension of the simple linear regression model, which considers only one independent variable. Multiple linear regression allows you to account for the combined effect of multiple predictors on the dependent variable. The multiple linear regression model can be represented by the equation:\n",
    "\n",
    "\\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p + \\varepsilon\\]\n",
    "\n",
    "Where:\n",
    "- \\(Y\\) is the dependent variable you're trying to predict.\n",
    "- \\(X_1, X_2, \\ldots, X_p\\) are the independent variables (predictors).\n",
    "- \\(\\beta_0\\) is the intercept (the value of \\(Y\\) when all independent variables are zero).\n",
    "- \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients, representing the change in \\(Y\\) associated with a one-unit change in each \\(X\\) variable while holding other variables constant.\n",
    "- \\(\\varepsilon\\) is the error term, representing unexplained variance in \\(Y\\).\n",
    "\n",
    "Differences between Multiple Linear Regression and Simple Linear Regression:\n",
    "\n",
    "1. **Number of Independent Variables**:\n",
    "   - In simple linear regression, there is only one independent variable.\n",
    "   - In multiple linear regression, there are two or more independent variables.\n",
    "\n",
    "2. **Model Complexity**:\n",
    "   - Simple linear regression models a linear relationship between two variables, which can be visualized as a straight line in two-dimensional space.\n",
    "   - Multiple linear regression models a hyperplane in a higher-dimensional space, which accounts for the interaction of multiple independent variables.\n",
    "\n",
    "3. **Interpretation of Coefficients**:\n",
    "   - In simple linear regression, there is one coefficient representing the change in the dependent variable for a one-unit change in the single independent variable.\n",
    "   - In multiple linear regression, each independent variable has its coefficient, which indicates how much the dependent variable changes for a one-unit change in that specific independent variable while holding the other variables constant. This allows you to assess the unique contribution of each predictor.\n",
    "\n",
    "4. **Model Assumptions**:\n",
    "   - Both simple and multiple linear regression rely on similar assumptions, such as linearity, independence of errors, homoscedasticity, normality of errors, no or little multicollinearity, and zero conditional mean. However, the assumptions are extended and may be more complex in the multiple linear regression context due to the presence of multiple predictors.\n",
    "\n",
    "5. **Model Performance and Complexity**:\n",
    "   - Multiple linear regression models are more flexible and can capture more complex relationships, but they are also more prone to overfitting when including many predictors with limited data.\n",
    "   - Simple linear regression models are less complex and can be easier to interpret, but they may not capture the nuances in the data when the relationship is more intricate.\n",
    "\n",
    "Multiple linear regression is a valuable tool when you want to analyze how multiple independent variables jointly influence a single dependent variable, making it suitable for various real-world scenarios in fields such as economics, finance, and social sciences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29719703-d1f0-4e5f-beba-7713073662eb",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n",
    "\n",
    "Ans.: **Multicollinearity** is a common issue that can occur in multiple linear regression when two or more independent variables in the model are highly correlated with each other. It can make it difficult to assess the individual effect of each independent variable on the dependent variable, and it can lead to unstable and unreliable coefficient estimates. There are two main types of multicollinearity:\n",
    "\n",
    "1. **Perfect Multicollinearity**: This occurs when one independent variable is a perfect linear combination of others. For example, if you have two independent variables, X1 and X2, and you can express X2 as a constant times X1 (X2 = 2*X1), it leads to perfect multicollinearity.\n",
    "\n",
    "2. **High Multicollinearity**: This is the more common form of multicollinearity, where independent variables are highly correlated but not perfectly linearly related.\n",
    "\n",
    "**Detecting Multicollinearity**:\n",
    "\n",
    "You can detect multicollinearity through several methods:\n",
    "\n",
    "1. **Correlation Matrix**: Calculate the correlation coefficients between pairs of independent variables. High absolute values (close to 1) indicate strong correlations.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF)**: Calculate the VIF for each independent variable. The VIF of a variable measures how much its variance is inflated due to multicollinearity. A VIF greater than 1 suggests multicollinearity, and higher values indicate stronger multicollinearity.\n",
    "\n",
    "**Addressing Multicollinearity**:\n",
    "\n",
    "If you detect multicollinearity in your multiple linear regression model, here are some strategies to address it:\n",
    "\n",
    "1. **Remove or Combine Variables**: If you identify variables that are highly correlated, you can consider removing one of them or combining them into a single variable. However, be cautious about dropping variables with theoretical significance, as this may affect the interpretability of your model.\n",
    "\n",
    "2. **Feature Selection**: Use feature selection techniques like forward selection, backward elimination, or stepwise regression to select a subset of the most important variables for your model. These methods can help reduce multicollinearity by excluding less relevant variables.\n",
    "\n",
    "3. **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that can help transform correlated variables into a set of orthogonal (uncorrelated) variables known as principal components. You can then use these components in your regression model.\n",
    "\n",
    "4. **Regularization Techniques**: Ridge and Lasso regression are regularization techniques that can mitigate multicollinearity by adding a penalty to the magnitude of coefficients. Ridge regression, in particular, can be effective in reducing the impact of correlated variables.\n",
    "\n",
    "5. **Collect More Data**: In some cases, multicollinearity can be a result of a small sample size. Increasing your dataset's size can sometimes reduce the effect of multicollinearity.\n",
    "\n",
    "6. **Partial Correlation Analysis**: Instead of directly analyzing the correlation between variables, you can perform partial correlation analysis to determine the relationship between two variables while controlling for the influence of other variables. This can help identify which variables are genuinely correlated.\n",
    "\n",
    "Addressing multicollinearity is crucial for obtaining reliable and interpretable results in multiple linear regression. The choice of method depends on the specific circumstances of your dataset and the goals of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147cb37-0258-47a7-bfa4-a8d785b4e61f",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Ans.: **Polynomial regression** is a type of regression analysis used to model relationships between a dependent variable and one or more independent variables when the relationship is not linear but follows a polynomial form. Unlike linear regression, which assumes a linear relationship between the dependent and independent variables, polynomial regression allows for curves and nonlinear patterns in the data.\n",
    "\n",
    "In a polynomial regression model, the relationship is represented by a polynomial equation of a specified degree, often quadratic (degree 2) or cubic (degree 3), although higher degrees can also be used. The general form of a polynomial regression equation is as follows:\n",
    "\n",
    "\\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\ldots + \\beta_nX^n + \\varepsilon\\]\n",
    "\n",
    "Where:\n",
    "- \\(Y\\) is the dependent variable.\n",
    "- \\(X\\) is the independent variable.\n",
    "- \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n\\) are the coefficients to be estimated.\n",
    "- \\(n\\) is the degree of the polynomial, indicating how many terms are included.\n",
    "- \\(\\varepsilon\\) is the error term, representing unexplained variance.\n",
    "\n",
    "Differences between Polynomial Regression and Linear Regression:\n",
    "\n",
    "1. **Linearity**:\n",
    "   - Linear Regression: Assumes a linear relationship between the dependent and independent variables, fitting a straight line to the data.\n",
    "   - Polynomial Regression: Allows for curved relationships and fits a polynomial function to the data, capturing nonlinear patterns.\n",
    "\n",
    "2. **Model Complexity**:\n",
    "   - Linear Regression: Simpler model with only two parameters (intercept and slope) for each independent variable.\n",
    "   - Polynomial Regression: Can have a higher degree of complexity, especially when using higher-degree polynomials, leading to more parameters.\n",
    "\n",
    "3. **Overfitting**:\n",
    "   - Linear Regression: Tends to underfit when the relationship between variables is nonlinear.\n",
    "   - Polynomial Regression: More flexible and can capture intricate patterns, but it's susceptible to overfitting, especially with high-degree polynomials. Regularization techniques like ridge or lasso regression can help address this issue.\n",
    "\n",
    "4. **Interpretability**:\n",
    "   - Linear Regression: Coefficients represent the linear change in the dependent variable for a one-unit change in the independent variable.\n",
    "   - Polynomial Regression: Coefficients have a less direct and intuitive interpretation, as they represent the effect of changes in the independent variable and its powers on the dependent variable.\n",
    "\n",
    "Polynomial regression is useful when the relationship between variables is clearly nonlinear and linear regression would not adequately capture the data's behavior. It can be applied in various fields, including physics, biology, economics, and engineering, to model complex phenomena. However, selecting the appropriate degree of the polynomial and addressing overfitting are important considerations in polynomial regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4047798-e3ab-4be6-8420-763b995bfd11",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Ans.: **Advantages of Polynomial Regression Compared to Linear Regression**:\n",
    "\n",
    "1. **Flexibility**: Polynomial regression can model complex, nonlinear relationships between the dependent and independent variables, while linear regression assumes a linear relationship. This allows polynomial regression to capture a wider range of patterns in the data.\n",
    "\n",
    "2. **Improved Fit**: In cases where the relationship is nonlinear, polynomial regression can provide a better fit to the data compared to linear regression. This can result in more accurate predictions and reduced residuals.\n",
    "\n",
    "3. **Increased R-squared Value**: The R-squared value (coefficient of determination) is often higher in polynomial regression models compared to linear regression models, indicating a better fit to the data.\n",
    "\n",
    "**Disadvantages of Polynomial Regression Compared to Linear Regression**:\n",
    "\n",
    "1. **Overfitting**: Polynomial regression models, especially those with high-degree polynomials, are prone to overfitting. They can become overly complex and perform poorly on new, unseen data. Regularization techniques like ridge or lasso regression can help mitigate this issue.\n",
    "\n",
    "2. **Reduced Interpretability**: The coefficients in polynomial regression have a less intuitive interpretation than those in linear regression. It can be challenging to explain the effect of variables, especially in high-degree polynomial models.\n",
    "\n",
    "3. **Increased Model Complexity**: Higher-degree polynomial models have more parameters, which makes them computationally more intensive and requires a larger amount of data to estimate the coefficients reliably.\n",
    "\n",
    "**Situation Where Polynomial Regression May Be Preferred**:\n",
    "\n",
    "Polynomial regression is a valuable choice in various situations:\n",
    "\n",
    "1. **Nonlinear Relationships**: When it's clear that the relationship between the dependent and independent variables is nonlinear, polynomial regression can capture the underlying patterns better than linear regression.\n",
    "\n",
    "2. **Curved Trends**: If you have data that exhibits curvature or nonlinear trends, such as exponential growth, quadratic decay, or periodic behavior, polynomial regression can be a suitable choice.\n",
    "\n",
    "3. **High-Degree Polynomials**: In some cases, particularly in experimental or scientific research, a high-degree polynomial model may be appropriate for modeling complex phenomena.\n",
    "\n",
    "4. **Improved Fit**: When the goal is to achieve the best possible fit to the data and the priority is accurate predictions over model interpretability, polynomial regression can be preferred.\n",
    "\n",
    "5. **Understanding Higher-Order Effects**: In situations where you want to explore the effect of not only the linear relationship but also higher-order effects of the independent variable on the dependent variable, polynomial regression can provide insight.\n",
    "\n",
    "It's important to note that selecting the degree of the polynomial in a polynomial regression model requires careful consideration. Too high a degree can lead to overfitting, while too low a degree may result in underfitting. Cross-validation techniques and model evaluation are crucial to determining the appropriate complexity of the model. Regularization methods, feature selection, and feature engineering can also be useful in managing the trade-off between complexity and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ed9ca-51cc-47af-a15f-7f341af8e071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
