{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93287888-b039-4c37-a4a7-ecd47a934395",
   "metadata": {},
   "source": [
    "# Na√Øve bayes-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adbae9-3f16-4200-9c83-f28400098e91",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?\n",
    "Ans.: Bayes' theorem is a fundamental concept in probability theory named after the Reverend Thomas Bayes. It describes the probability of an event based on prior knowledge or conditions that might be related to the event. Essentially, Bayes' theorem provides a way to revise or update probabilities based on new evidence.\n",
    "\n",
    "In its simplest form, Bayes' theorem states that the probability of event A given that event B has occurred (denoted as \\( P(A|B) \\)) is equal to the probability of event B given that event A has occurred (denoted as \\( P(B|A) \\)) multiplied by the probability of event A (denoted as \\( P(A) \\)), divided by the probability of event B (denoted as \\( P(B) \\)).\n",
    "\n",
    "Mathematically, Bayes' theorem can be expressed as:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "This theorem is widely used in various fields including statistics, machine learning, and artificial intelligence for tasks such as classification, prediction, and inference. It provides a formal framework for reasoning under uncertainty and updating beliefs based on new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b5026-89bf-4240-8f7c-cda83bc247b2",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?\n",
    "Ans.: The formula for Bayes' theorem is:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the probability of event A given that event B has occurred.\n",
    "- \\( P(B|A) \\) is the probability of event B given that event A has occurred.\n",
    "- \\( P(A) \\) and \\( P(B) \\) are the probabilities of events A and B, respectively.\n",
    "\n",
    "This formula provides a way to update or revise probabilities based on new evidence or information. It is widely used in various fields including statistics, machine learning, and artificial intelligence for tasks such as classification, prediction, and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3de8a-add4-4028-81ea-73515e1b3570",
   "metadata": {},
   "source": [
    "Q3. How is Bayes' theorem used in practice?\n",
    "Ans.: Bayes' theorem is used in practice in various fields and applications. Some common uses include:\n",
    "\n",
    "1. **Statistical Inference**: Bayes' theorem is used to update probabilities based on observed data, making it a fundamental tool in statistical inference. It allows researchers to estimate unknown parameters, make predictions, and quantify uncertainty.\n",
    "\n",
    "2. **Machine Learning and Data Science**: In machine learning, Bayes' theorem is used in Bayesian inference to update the probabilities of hypotheses or model parameters based on observed data. It is employed in Bayesian models such as Naive Bayes classifiers, Bayesian networks, and Bayesian regression.\n",
    "\n",
    "3. **Spam Filtering**: Bayes' theorem is applied in spam filtering algorithms to classify emails as spam or legitimate based on the probability of certain words or features occurring in spam or non-spam emails.\n",
    "\n",
    "4. **Medical Diagnosis**: Bayes' theorem is used in medical diagnosis to calculate the probability of a disease given certain symptoms or test results. It helps physicians make informed decisions by updating the probability of a disease based on observed evidence.\n",
    "\n",
    "5. **Document Classification**: In natural language processing, Bayes' theorem is used for document classification tasks such as sentiment analysis, topic modeling, and text categorization. It allows algorithms to classify documents into predefined categories based on the occurrence of specific words or features.\n",
    "\n",
    "6. **Fault Diagnosis**: Bayes' theorem is applied in fault diagnosis systems to identify and localize faults in complex systems such as manufacturing processes, automotive systems, and telecommunications networks. It helps engineers and technicians diagnose problems by analyzing sensor data and system behavior.\n",
    "\n",
    "Overall, Bayes' theorem provides a principled framework for reasoning under uncertainty and updating beliefs based on new evidence. Its versatility and applicability make it a powerful tool in a wide range of fields and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbec23e-3a29-401b-95c1-dbc2ddd3914f",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "Ans.: Bayes' theorem and conditional probability are closely related concepts in probability theory. Conditional probability is a fundamental concept that deals with the probability of an event occurring given that another event has already occurred. Bayes' theorem provides a way to update or revise probabilities based on new evidence or information.\n",
    "\n",
    "The relationship between Bayes' theorem and conditional probability can be understood through the formula for Bayes' theorem:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Here's how they are connected:\n",
    "\n",
    "1. **Bayes' Theorem Involves Conditional Probability**: Bayes' theorem uses conditional probabilities in its formula. The probability \\( P(A|B) \\) represents the probability of event A occurring given that event B has occurred. Similarly, \\( P(B|A) \\) represents the probability of event B occurring given that event A has occurred.\n",
    "\n",
    "2. **Updating Probabilities Using Conditional Probabilities**: Bayes' theorem provides a systematic way to update or revise probabilities based on new evidence. It does this by incorporating conditional probabilities: \\( P(B|A) \\) is the likelihood of observing evidence B given that the hypothesis A is true, and \\( P(A|B) \\) is the updated probability of A given the observation of evidence B.\n",
    "\n",
    "3. **Bayesian Inference and Conditional Probability**: In Bayesian inference, Bayes' theorem is used to update prior beliefs (expressed as probabilities) about hypotheses or model parameters based on observed data. This updating process involves conditional probabilities, where the probability of a hypothesis or parameter is updated given the observed data.\n",
    "\n",
    "In summary, Bayes' theorem and conditional probability are intertwined concepts in probability theory. Bayes' theorem utilizes conditional probabilities to update probabilities based on new evidence, making it a powerful tool for reasoning under uncertainty and updating beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548b342b-0c65-4466-bbcf-5466b29127ba",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "Ans.: Choosing the appropriate type of Naive Bayes classifier for a given problem depends on several factors, including the nature of the features in the dataset and the assumptions you're willing to make about their distribution. Here's a general guideline for selecting the type of Naive Bayes classifier:\n",
    "\n",
    "1. **Nature of Features**:\n",
    "   - **Continuous Features**: If the features in the dataset are continuous (i.e., real-valued), Gaussian Naive Bayes is often a good choice. It assumes that the features follow a Gaussian (normal) distribution.\n",
    "   - **Discrete Features**: If the features are discrete (e.g., word counts, presence/absence of features), Multinomial Naive Bayes or Bernoulli Naive Bayes can be suitable.\n",
    "   - **Mixed Features**: If the dataset contains a mix of continuous and discrete features, you may need to preprocess the data or use a hybrid approach.\n",
    "\n",
    "2. **Assumptions about Feature Independence**:\n",
    "   - **Multinomial Naive Bayes**: Assumes that features are generated from a multinomial distribution. It's commonly used for text classification tasks where features represent word counts or frequencies.\n",
    "   - **Bernoulli Naive Bayes**: Assumes that features are binary-valued (e.g., presence or absence of features). It's suitable for problems with binary features or where only the presence or absence of features matters.\n",
    "   - **Gaussian Naive Bayes**: Assumes that features follow a Gaussian distribution. It's appropriate when dealing with continuous features that can be assumed to be normally distributed.\n",
    "\n",
    "3. **Dataset Size**:\n",
    "   - **Small Datasets**: With small datasets, simpler models like Multinomial or Bernoulli Naive Bayes may perform better due to lower risk of overfitting.\n",
    "   - **Large Datasets**: For large datasets, Gaussian Naive Bayes can be efficient and scalable, especially if the features are approximately normally distributed.\n",
    "\n",
    "4. **Performance Evaluation**:\n",
    "   - **Cross-Validation**: Use techniques like cross-validation to evaluate the performance of different Naive Bayes classifiers on your dataset.\n",
    "   - **Model Assumptions**: Consider whether the assumptions made by each Naive Bayes classifier (e.g., feature independence) hold true for your dataset.\n",
    "\n",
    "5. **Domain Knowledge**:\n",
    "   - **Understanding the Problem**: Consider domain-specific knowledge and insights about the problem domain when selecting the Naive Bayes classifier. For example, knowledge about the distribution of features or the relevance of feature independence can inform your choice.\n",
    "\n",
    "In summary, the choice of Naive Bayes classifier depends on a combination of factors including the nature of features, assumptions about feature independence, dataset size, performance evaluation, and domain knowledge. It's often beneficial to try multiple types of Naive Bayes classifiers and compare their performance to choose the one that best suits your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23955d-4894-42a5-85c1-0216bea68032",
   "metadata": {},
   "source": [
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "\n",
    "Ans.: To classify a new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the probabilities for each class and then compare them to determine the most likely class. \n",
    "\n",
    "Given the frequency table provided, let's calculate the necessary probabilities:\n",
    "\n",
    "1. Calculate the total number of instances for each class:\n",
    "   - Total instances for class A = \\(3 + 3 + 4 + 4 + 3 + 3 + 3 = 23\\)\n",
    "   - Total instances for class B = \\(2 + 2 + 1 + 2 + 2 + 2 + 3 = 14\\)\n",
    "\n",
    "2. Calculate the prior probabilities for each class:\n",
    "   - \\( P(A) = \\frac{23}{23 + 14} = \\frac{23}{37} \\)\n",
    "   - \\( P(B) = \\frac{14}{23 + 14} = \\frac{14}{37} \\)\n",
    "\n",
    "3. Calculate the conditional probabilities for each feature value given each class:\n",
    "   - For class A:\n",
    "     - \\( P(X1=3 | A) = \\frac{4}{23} \\)\n",
    "     - \\( P(X2=4 | A) = \\frac{3}{23} \\)\n",
    "   - For class B:\n",
    "     - \\( P(X1=3 | B) = \\frac{1}{14} \\)\n",
    "     - \\( P(X2=4 | B) = \\frac{3}{14} \\)\n",
    "\n",
    "4. Use the Naive Bayes classifier formula to calculate the posterior probabilities for each class:\n",
    "   - For class A:\n",
    "     \\[ P(A | X1=3, X2=4) = \\frac{P(X1=3 | A) \\cdot P(X2=4 | A) \\cdot P(A)}{P(X1=3) \\cdot P(X2=4)} \\]\n",
    "   - For class B:\n",
    "     \\[ P(B | X1=3, X2=4) = \\frac{P(X1=3 | B) \\cdot P(X2=4 | B) \\cdot P(B)}{P(X1=3) \\cdot P(X2=4)} \\]\n",
    "\n",
    "5. Compare the posterior probabilities for each class and classify the new instance into the class with the highest probability.\n",
    "\n",
    "Let's perform the calculations:\n",
    "\n",
    "\\[ P(A | X1=3, X2=4) = \\frac{\\frac{4}{23} \\times \\frac{3}{23} \\times \\frac{23}{37}}{P(X1=3) \\times P(X2=4)} \\]\n",
    "\n",
    "\\[ P(B | X1=3, X2=4) = \\frac{\\frac{1}{14} \\times \\frac{3}{14} \\times \\frac{14}{37}}{P(X1=3) \\times P(X2=4)} \\]\n",
    "\n",
    "After calculating the probabilities, we'll compare \\( P(A | X1=3, X2=4) \\) and \\( P(B | X1=3, X2=4) \\) to determine which class has the higher probability and thus classify the new instance accordingly. Let's proceed with the calculations.\n",
    "\n",
    "Let's first calculate \\( P(X1=3) \\) and \\( P(X2=4) \\) as they are needed in the denominator of both posterior probabilities:\n",
    "\n",
    "\\[ P(X1=3) = P(X1=3 | A) \\cdot P(A) + P(X1=3 | B) \\cdot P(B) \\]\n",
    "\\[ P(X1=3) = \\frac{4}{23} \\cdot \\frac{23}{37} + \\frac{1}{14} \\cdot \\frac{14}{37} \\]\n",
    "\\[ P(X1=3) = \\frac{4}{37} + \\frac{1}{37} \\]\n",
    "\\[ P(X1=3) = \\frac{5}{37} \\]\n",
    "\n",
    "\\[ P(X2=4) = P(X2=4 | A) \\cdot P(A) + P(X2=4 | B) \\cdot P(B) \\]\n",
    "\\[ P(X2=4) = \\frac{3}{23} \\cdot \\frac{23}{37} + \\frac{3}{14} \\cdot \\frac{14}{37} \\]\n",
    "\\[ P(X2=4) = \\frac{3}{37} + \\frac{3}{37} \\]\n",
    "\\[ P(X2=4) = \\frac{6}{37} \\]\n",
    "\n",
    "Now, we can calculate the posterior probabilities:\n",
    "\n",
    "\\[ P(A | X1=3, X2=4) = \\frac{\\frac{4}{23} \\cdot \\frac{3}{23} \\cdot \\frac{23}{37}}{\\frac{5}{37} \\cdot \\frac{6}{37}} \\]\n",
    "\\[ P(A | X1=3, X2=4) = \\frac{4}{5} \\cdot \\frac{3}{6} \\]\n",
    "\\[ P(A | X1=3, X2=4) = \\frac{2}{5} \\]\n",
    "\n",
    "\\[ P(B | X1=3, X2=4) = \\frac{\\frac{1}{14} \\cdot \\frac{3}{14} \\cdot \\frac{14}{37}}{\\frac{5}{37} \\cdot \\frac{6}{37}} \\]\n",
    "\\[ P(B | X1=3, X2=4) = \\frac{1}{5} \\cdot \\frac{3}{6} \\]\n",
    "\\[ P(B | X1=3, X2=4) = \\frac{1}{10} \\]\n",
    "\n",
    "Since \\( P(A | X1=3, X2=4) > P(B | X1=3, X2=4) \\), we would predict that the new instance with features X1 = 3 and X2 = 4 belongs to class A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df058d4-543d-487f-b05d-5cda94020fa1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
