{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4be6afd-6fd3-41e7-bb3a-a887ca2cb639",
   "metadata": {},
   "source": [
    "# Na√Øve bayes-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d8fe7-7c54-428b-b87c-6b9fb32ce380",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "Ans.: To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use conditional probability. Let's denote the following events:\n",
    "\n",
    "- Event \\( S \\): An employee is a smoker.\n",
    "- Event \\( H \\): An employee uses the health insurance plan.\n",
    "\n",
    "We want to find \\( P(S|H) \\), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "\\[ P(S|H) = \\frac{P(H|S) \\cdot P(S)}{P(H)} \\]\n",
    "\n",
    "Given:\n",
    "- \\( P(H) \\), the probability that an employee uses the health insurance plan, is 70% or 0.70.\n",
    "- \\( P(S|H) \\), the probability that an employee uses the health insurance plan given that they are a smoker, is 40% or 0.40.\n",
    "- \\( P(S) \\), the overall probability that an employee is a smoker, is not given directly but we can calculate it using the Law of Total Probability.\n",
    "\n",
    "The Law of Total Probability states:\n",
    "\n",
    "\\[ P(S) = P(S|H) \\cdot P(H) + P(S|H') \\cdot P(H') \\]\n",
    "\n",
    "where \\( H' \\) represents the event that an employee does not use the health insurance plan, and \\( P(H') = 1 - P(H) \\) is the probability that an employee does not use the health insurance plan.\n",
    "\n",
    "Given that we know \\( P(H) = 0.70 \\) and \\( P(S|H) = 0.40 \\), we can calculate \\( P(S) \\) as follows:\n",
    "\n",
    "\\[ P(S) = 0.40 \\cdot 0.70 + P(S|H') \\cdot (1 - 0.70) \\]\n",
    "\n",
    "Now, \\( P(S|H') \\) is the probability that an employee is a smoker given that he/she does not use the health insurance plan. This value is not directly given in the problem statement. If we assume that the probability of being a smoker is independent of whether the employee uses the health insurance plan or not, then \\( P(S|H') = P(S) \\). \n",
    "\n",
    "Substituting this assumption into the equation, we get:\n",
    "\n",
    "\\[ P(S) = 0.40 \\cdot 0.70 + P(S) \\cdot (1 - 0.70) \\]\n",
    "\n",
    "\\[ P(S) = 0.40 \\cdot 0.70 + P(S) \\cdot 0.30 \\]\n",
    "\n",
    "\\[ P(S) = 0.28 + 0.30 \\cdot P(S) \\]\n",
    "\n",
    "\\[ 0.70 \\cdot P(S) = 0.28 \\]\n",
    "\n",
    "\\[ P(S) = \\frac{0.28}{0.70} \\]\n",
    "\n",
    "\\[ P(S) = 0.4 \\]\n",
    "\n",
    "Now that we have \\( P(S) = 0.4 \\), we can substitute this value into Bayes' theorem to find \\( P(S|H) \\):\n",
    "\n",
    "\\[ P(S|H) = \\frac{0.40 \\cdot 0.70}{0.70} \\]\n",
    "\n",
    "\\[ P(S|H) = 0.40 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is \\( 0.40 \\), or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f6622b-4754-4a81-9a84-e77c2fabb1c5",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "Ans.: The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the assumptions they make about the distribution of feature probabilities and how they handle feature occurrences.\n",
    "\n",
    "1. **Feature Representation**:\n",
    "   - **Bernoulli Naive Bayes**: Assumes that features are binary-valued (i.e., present or absent). It considers only the presence or absence of a feature, ignoring the frequency of occurrence.\n",
    "   - **Multinomial Naive Bayes**: Assumes that features are represented as counts or frequencies (e.g., word counts in document classification). It considers the frequency of occurrence of each feature.\n",
    "\n",
    "2. **Feature Probability Distribution**:\n",
    "   - **Bernoulli Naive Bayes**: Models the distribution of features using a Bernoulli distribution. It calculates the probability of each feature being present or absent for each class.\n",
    "   - **Multinomial Naive Bayes**: Models the distribution of features using a multinomial distribution. It calculates the probability of observing each possible value of a feature (usually counts or frequencies) for each class.\n",
    "\n",
    "3. **Handling Missing Features**:\n",
    "   - **Bernoulli Naive Bayes**: Can handle missing features by treating them as absent, assuming that the absence of a feature provides useful information.\n",
    "   - **Multinomial Naive Bayes**: Typically cannot handle missing features directly because it relies on feature counts or frequencies. Missing features may need to be imputed or handled separately.\n",
    "\n",
    "4. **Applications**:\n",
    "   - **Bernoulli Naive Bayes**: Commonly used for text classification tasks where features represent the presence or absence of words (e.g., spam detection, sentiment analysis).\n",
    "   - **Multinomial Naive Bayes**: Widely used for text classification tasks where features represent word counts or frequencies (e.g., document categorization, topic modeling).\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the features in the dataset and the assumptions you're willing to make about their distribution. If features are binary-valued and the frequency of occurrence is not important, Bernoulli Naive Bayes may be more appropriate. If features are represented as counts or frequencies, Multinomial Naive Bayes may be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d75a2-d2be-4ca2-9c7f-3f12682386ab",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "Ans.: Bernoulli Naive Bayes handles missing values by treating them as absent features. Since Bernoulli Naive Bayes assumes that features are binary-valued (i.e., present or absent), missing values are treated as if the corresponding feature is not present for the given instance.\n",
    "\n",
    "When calculating probabilities for each class, Bernoulli Naive Bayes considers the presence or absence of each feature. If a feature is missing for a particular instance, it is assumed to be absent, and its absence contributes to the calculation of probabilities in the same way as if it were explicitly absent.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes handles missing values in practice:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - During the training phase, Bernoulli Naive Bayes learns the probability of each feature being present or absent for each class based on the training data.\n",
    "   - If a feature is missing for an instance in the training data, it is treated as absent, and the absence of that feature contributes to the estimation of probabilities for each class.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - When making predictions on new instances with missing values, Bernoulli Naive Bayes treats the missing values as absent features.\n",
    "   - The model calculates the probabilities for each class based on the presence or absence of features, including the missing ones.\n",
    "\n",
    "Overall, Bernoulli Naive Bayes handles missing values implicitly by assuming that missing features are absent. This approach allows the model to make predictions without requiring imputation or special handling of missing values explicitly. However, it's essential to consider how missing values may impact the performance and reliability of the model, especially if missingness is related to the outcome or other features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd7525-6a91-4169-a7a3-74a5c5f7e011",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "Ans.: Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes features follow a Gaussian (normal) distribution. While it's commonly used for binary or two-class classification problems, it can also be extended to handle multi-class classification tasks.\n",
    "\n",
    "In multi-class classification, Gaussian Naive Bayes assigns each instance to one of several classes based on the probability distribution of features for each class. It calculates the probability of each class given the observed features using Bayes' theorem and selects the class with the highest posterior probability as the predicted class.\n",
    "\n",
    "Here's how Gaussian Naive Bayes can be used for multi-class classification:\n",
    "\n",
    "1. **Model Training**:\n",
    "   - During the training phase, Gaussian Naive Bayes estimates the parameters of the Gaussian distribution (mean and variance) for each feature in each class using the training data.\n",
    "   - For each class, it calculates the mean and variance of each feature based on the instances belonging to that class.\n",
    "\n",
    "2. **Prediction**:\n",
    "   - When making predictions on new instances, Gaussian Naive Bayes calculates the probability of each class given the observed feature values.\n",
    "   - It uses the Gaussian probability density function to calculate the likelihood of the observed feature values given each class's parameter estimates.\n",
    "   - It then applies Bayes' theorem to calculate the posterior probability of each class given the observed features.\n",
    "   - Finally, it selects the class with the highest posterior probability as the predicted class for the new instance.\n",
    "\n",
    "3. **Handling Multi-Class**:\n",
    "   - Gaussian Naive Bayes can handle multi-class classification by extending the binary classification approach to multiple classes.\n",
    "   - It calculates the posterior probability for each class and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can indeed be used for multi-class classification by extending its binary classification approach to handle multiple classes. However, it's essential to ensure that the assumption of features following a Gaussian distribution holds reasonably well for the dataset being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeb5d20-3dd8-41c7-85ab-69f5ab0ef660",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4cf275-1961-40c2-bd77-3cc458315d42",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
